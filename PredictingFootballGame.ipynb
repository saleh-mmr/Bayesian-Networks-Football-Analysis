{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q_a6dqlNt9XK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Predicting football game outcome through Bayesian reasoning**\n",
        "\n",
        "\n",
        "**Saleh Mir Mohammad Rezaei** \\\n",
        "Project for Fundamentals of AI course - module 3\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ba5ndw7D2Lkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It aims to predict football game outcomes using Bayesian reasoning techniques. The match data and relevant features are preprocessed, and probabilistic models will be applied to classify match results. The goal is to leverage statistical insights for accurate and interpretable predictions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AxopZlK83rKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. Data Preprocessing\n",
        "The Football Match Statistics Dataset from Kaggle is used for this project. This dataset contains detailed match statistics for various football games. It consists of approximately 100,000 rows and 91 columns, offering rich data to analyze team and player performance across multiple dimensions. This dataset includes matches from 18 leagues, consisting of 3 leagues from each of the 6 countries.\n",
        "\n"
      ],
      "metadata": {
        "id": "5nF5IdvB5rWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "M3jrXamsDAkx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The dataset has been loaded, and an additional column has been introduced to represent the outcome of the match for the host team. Out of over 90 columns in the dataset, 14 features have been selected for the classification of match results.\n",
        "\n",
        "\n",
        "\n",
        "> Because null values can disrupt splitting criteria, reduce model performance, and affect interpretability, it is essential to address them in the dataset. For numeric features, we replace null values with the mean, and for categorical features, we use the mode. This ensures the decision tree can effectively learn patterns without being impacted by missing data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MYaEfndkBqwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"Football.csv\", sep=',', header=0)\n",
        "\n",
        "# Create match_outcome column and set value\n",
        "data['match_outcome'] = data.apply(\n",
        "          lambda row: 'win'\n",
        "                      if row['home_score'] > row['away_score']\n",
        "                      else ('lose' if row['home_score'] < row['away_score']\n",
        "                      else 'draw'), axis=1)"
      ],
      "metadata": {
        "id": "VF4_0lHUC-ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant features\n",
        "selected_columns = [\n",
        "    'home_team', 'away_team', 'Ball_Possession_Home','Shots_on_Goal_Home',\n",
        "    'Shots_on_Goal_Away', 'Yellow_Cards_Home', 'Yellow_Cards_Away',\n",
        "    'first_half', 'Red_Cards_Home', 'Red_Cards_Away', 'Goal_Attempts_Home',\n",
        "    'Goal_Attempts_Away', 'match_outcome']\n",
        "\n",
        "data = data[selected_columns]"
      ],
      "metadata": {
        "id": "gxISzo7PDDR8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show number of null values for each feature\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "jNm8vDTZD6qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values with mean of the column for numeric features\n",
        "columns_to_fill = [\n",
        "    'Shots_on_Goal_Home', 'Shots_on_Goal_Away', 'Yellow_Cards_Home',\n",
        "    'Yellow_Cards_Away', 'Red_Cards_Home', 'Red_Cards_Away',\n",
        "    'Goal_Attempts_Home', 'Goal_Attempts_Away'\n",
        "]\n",
        "\n",
        "for col in columns_to_fill:\n",
        "    data[col].fillna(data[col].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "o-dTagIVEHS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace null values with mode of the column for categorical features\n",
        "def valid_score_format(score):\n",
        "    try:\n",
        "        x, y = map(int, score.split(' - '))\n",
        "        return score  # If valid, keep the original score\n",
        "    except:\n",
        "        return '0 - 0'  # If invalid, replace with '0 - 0'\n",
        "\n",
        "data['first_half'] = data['first_half'].apply(valid_score_format)\n",
        "\n",
        "# Determine the result directly by splitting and comparing the values\n",
        "def determine_result(score):\n",
        "    x, y = map(int, score.split(' - '))\n",
        "    if x > y:\n",
        "        return 'win'\n",
        "    elif x == y:\n",
        "        return 'draw'\n",
        "    else:\n",
        "        return 'lose'\n",
        "\n",
        "data['first_half'] = data['first_half'].apply(determine_result)"
      ],
      "metadata": {
        "id": "SLPWojlyEae9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove '%' and convert to float and for null values use mean\n",
        "data['Ball_Possession_Home'] = data['Ball_Possession_Home'].str.rstrip('%').astype(float)\n",
        "data['Ball_Possession_Home'].fillna(data['Ball_Possession_Home'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "Ymps-s4TF7_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show number of null values\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "MGfkvieauXif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Several data preprocessing steps were undertaken to enhance the dataset's suitability for effective analysis and subsequent modeling. These steps include **scaling of numerical features** and **encoding of categorical variables**, and the establishment of consistent feature representations. MinMaxScaler and LabelEncoder from the scikit-learn library are used for this purpose."
      ],
      "metadata": {
        "id": "Iukga_01ul7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding of categorical variables\n",
        "\n",
        "# Strip extra whitespace and special characters\n",
        "data['home_team'] = data['home_team'].str.strip()\n",
        "data['away_team'] = data['away_team'].str.strip()\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "team_encoder = LabelEncoder()\n",
        "\n",
        "# Fit on all unique team names (combining both columns)\n",
        "all_teams = pd.concat([data['home_team'], data['away_team']]).unique()\n",
        "team_encoder.fit(all_teams)\n",
        "\n",
        "# Transform both columns\n",
        "data['home_team'] = team_encoder.transform(data['home_team'])\n",
        "data['away_team'] = team_encoder.transform(data['away_team'])\n",
        "\n",
        "# Encode match outcome and first half\n",
        "outcome_encoder = LabelEncoder()\n",
        "data['match_outcome'] = outcome_encoder.fit_transform(data['match_outcome'])\n",
        "data['first_half'] = outcome_encoder.fit_transform(data['first_half'])"
      ],
      "metadata": {
        "id": "tI5Ubdoh7W-7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling of numerical features\n",
        "numerical_columns = [\n",
        "    'Goal_Attempts_Home', 'Goal_Attempts_Away', 'Red_Cards_Home',\n",
        "    'Shots_on_Goal_Home', 'Shots_on_Goal_Away', 'Red_Cards_Away',\n",
        "    'Ball_Possession_Home', 'Goal_Attempts_Home', 'Goal_Attempts_Away',\n",
        "    'Yellow_Cards_Home', 'Yellow_Cards_Away']\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
        "\n",
        "# Discretize continuous variables into bins\n",
        "data['Ball_Possession_Home'] = pd.cut(data['Ball_Possession_Home'], bins=10, labels=False)\n",
        "\n",
        "data['Shots_on_Goal_Home'] = pd.cut(data['Shots_on_Goal_Home'], bins=10, labels=False)\n",
        "data['Shots_on_Goal_Away'] = pd.cut(data['Shots_on_Goal_Away'], bins=10, labels=False)\n",
        "\n",
        "data['Yellow_Cards_Home'] = pd.cut(data['Yellow_Cards_Home'], bins=10, labels=False)\n",
        "data['Yellow_Cards_Away'] = pd.cut(data['Yellow_Cards_Away'], bins=10, labels=False)\n",
        "\n",
        "data['Red_Cards_Home'] = pd.cut(data['Red_Cards_Home'], bins=5, labels=False)\n",
        "data['Red_Cards_Away'] = pd.cut(data['Red_Cards_Away'], bins=5, labels=False)\n",
        "\n",
        "data['Goal_Attempts_Home'] = pd.cut(data['Goal_Attempts_Home'], bins=10, labels=False)\n",
        "data['Goal_Attempts_Away'] = pd.cut(data['Goal_Attempts_Away'], bins=10, labels=False)"
      ],
      "metadata": {
        "id": "jp0VOLUN-FHh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">The processed dataset is saved to a CSV file named \"processed_data.csv\" using the to_csv method. Preprocessing has been finalized by exporting the cleaned and prepared data for forthcoming step.\n",
        "\n"
      ],
      "metadata": {
        "id": "mzIl98Y_-_Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the processed data\n",
        "data.to_csv(\"processed_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "OAnMY26a-eMf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Bayesian Networks: Football Match Analysis\n",
        "\n",
        "Bayesian Networks are explored using a dataset on football matches. Multiple Bayesian Network models are constructed, compared, and evaluated based on their structure, scoring metrics, and inference results. Hypotheses are tested through parameter sensitivity analyses and evidence-based scenario simulations, providing valuable insights into the relationships and dependencies within the dataset."
      ],
      "metadata": {
        "id": "l8ePWZOcE_VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pgmpy"
      ],
      "metadata": {
        "id": "46Tsa7z5PMDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.estimators import TreeSearch, HillClimbSearch, BDeuScore\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "from pgmpy.inference import VariableElimination\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "B_wSv7UhJm-w"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading and Exploration**\n",
        "> Various tools were utilized in this project to facilitate the analysis and modeling of Bayesian Networks. The pgmpy library was employed for Bayesian Network modeling and inference, while matplotlib and networkx were used to visualize network structures. Additionally, pandas was applied for data handling and preprocessing tasks.\n",
        "\n",
        "\n",
        "> The dataset was thoroughly explored to understand its features and distributions. This process involved loading the dataset, displaying summary statistics, and visualizing data distributions to gain insights and prepare the data for further analysis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jcdotNlhIJB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the processed data\n",
        "data = pd.read_csv(\"processed_data.csv\")"
      ],
      "metadata": {
        "id": "gH8Jqu0jFJen"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing teams columns and any rows with missing values\n",
        "data = data.drop(columns=['home_team', 'away_team']).dropna()"
      ],
      "metadata": {
        "id": "SYnnJGpDLbIh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Summary\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "Ci71BMfZLeWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First few rows of the dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "HolsiblsMXI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data distribution (optional)\n",
        "data.hist(bins=15, figsize=(15, 10))\n",
        "plt.suptitle(\"Dataset Feature Distributions\")\n",
        "plt.savefig(\"data_distributions.png\")"
      ],
      "metadata": {
        "id": "vPBJaJs3MnoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Bayesian Network Modeling**\n",
        "> Bayesian Network models were constructed and compared to analyze the dataset's underlying dependencies. Three models were developed: a Manual Model, designed based on domain knowledge; a TreeSearch Model, learned using the TreeSearch algorithm; and a Hill Climb Model, learned through the Hill Climb search algorithm with the BDeu scoring metric.\n",
        "\n"
      ],
      "metadata": {
        "id": "XX1KKOzRNTf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the manual model structure\n",
        "manual_model = BayesianNetwork([\n",
        "    ('Goal_Attempts_Home', 'Shots_on_Goal_Home'),\n",
        "    ('Goal_Attempts_Away', 'Shots_on_Goal_Away'),\n",
        "    ('Ball_Possession_Home', 'match_outcome'),\n",
        "    ('Shots_on_Goal_Home', 'match_outcome'),\n",
        "    ('Shots_on_Goal_Away', 'match_outcome'),\n",
        "    ('Yellow_Cards_Home', 'Red_Cards_Home'),\n",
        "    ('Yellow_Cards_Away', 'Red_Cards_Away'),\n",
        "    ('first_half', 'match_outcome'),\n",
        "    ('Red_Cards_Home', 'match_outcome'),\n",
        "    ('Red_Cards_Away', 'match_outcome')\n",
        "])\n",
        "\n",
        "# TreeSearch-based model\n",
        "ts = TreeSearch(data, root_node='match_outcome')\n",
        "tree_model = BayesianNetwork(ts.estimate())\n",
        "\n",
        "# Hill Climb Search-based model\n",
        "scorer = BDeuScore(data)\n",
        "hc = HillClimbSearch(data)\n",
        "hc_model = BayesianNetwork(hc.estimate(scoring_method=scorer))\n",
        "\n",
        "# Visualize the models\n",
        "def visualize_model(model, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    nx.draw_networkx(model, pos=nx.circular_layout(model), node_size=3000, font_size=10)\n",
        "    plt.title(title)\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "visualize_model(manual_model, \"Manual Model\", \"manual_model.png\")\n",
        "visualize_model(tree_model, \"TreeSearch Model\", \"tree_model.png\")\n",
        "visualize_model(hc_model, \"Hill Climb Model\", \"hc_model.png\")"
      ],
      "metadata": {
        "id": "bTU16t2cO0rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Fitting and Scoring**\n",
        "\n",
        "\n",
        "> All Bayesian Network models were fitted to the dataset using MaximumLikelihoodEstimator to ensure accurate parameterization. Conditional Probability Tables (CPTs) were computed for each model, and BDeuscores were calculated to assess and compare the quality of the models. This process enabled an objective evaluation of the models' performance and alignment with the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "QJvaBLrFQjbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit CPTs\n",
        "models = [('Manual', manual_model), ('TreeSearch', tree_model), ('HillClimb', hc_model)]\n",
        "for name, model in models:\n",
        "    print(f\"Fitting CPTs for {name} model...\")\n",
        "    model.fit(data, estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "# Compare scores\n",
        "model_scores = {name: scorer.score(model) for name, model in models}\n",
        "print(\"\\nModel Scores:\")\n",
        "for name, score in model_scores.items():\n",
        "    print(f\"{name} Model: {score}\")"
      ],
      "metadata": {
        "id": "9twyqh_jP1c-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ec9500-122e-4042-ad42-058c5b87506a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting CPTs for Manual model...\n",
            "Fitting CPTs for TreeSearch model...\n",
            "Fitting CPTs for HillClimb model...\n",
            "\n",
            "Model Scores:\n",
            "Manual Model: -811605.9831872664\n",
            "TreeSearch Model: -727982.7569958973\n",
            "HillClimb Model: -686820.9781788085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node = \"first_half\"\n",
        "\n",
        "# Print CPDs for a node in all models\n",
        "for name, model in models:\n",
        "    print(f\"\\nCPD for {node} in {name} model:\")\n",
        "    try:\n",
        "        cpd = model.get_cpds(node)\n",
        "        print(cpd)\n",
        "    except ValueError:\n",
        "        print(f\"No CPD found for '{node}' in {name} model.\")"
      ],
      "metadata": {
        "id": "KfoLs-N3lITK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform independence checks and Markov blanket analysis\n",
        "nodes = ['Ball_Possession_Home', 'Shots_on_Goal_Home', 'Shots_on_Goal_Away',\n",
        "         'Yellow_Cards_Home', 'Red_Cards_Home','Yellow_Cards_Away', 'Red_Cards_Away',\n",
        "         'first_half', 'Goal_Attempts_Home', 'Goal_Attempts_Away', 'match_outcome']\n",
        "\n",
        "for name, model in models:\n",
        "    print(f\"\\nAnalysis for {name} model:\")\n",
        "    print(f\"Total independence assertions: {len(model.get_independencies().get_assertions())}\")\n",
        "    for node in nodes:\n",
        "        print(f\"Markov blanket of {node}: {model.get_markov_blanket(node)}\")"
      ],
      "metadata": {
        "id": "UNEIWuAcgejt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Inference and Evidence Scenarios**\n",
        "> The behavior of the Bayesian Network models was tested under various evidence scenarios to evaluate their performance. Evidence scenarios were defined, and inference was performed for each model. The results were compared across models to assess their consistency and ability to capture the underlying dependencies within the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QTxFIzOOURg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define inference scenarios\n",
        "evidence_scenarios = [\n",
        "    {'Ball_Possession_Home': 2, 'first_half': 1},\n",
        "    {'Yellow_Cards_Home': 2, 'Shots_on_Goal_Away': 0},\n",
        "    {'Goal_Attempts_Home': 4, 'Shots_on_Goal_Home': 3},\n",
        "    {'Goal_Attempts_Home': 1, 'Shots_on_Goal_Home': 3},\n",
        "]\n",
        "\n",
        "inferences = [(name, VariableElimination(model)) for name, model in models]\n",
        "\n",
        "# Perform inference\n",
        "for idx, evidence in enumerate(evidence_scenarios):\n",
        "    print(f\"\\nScenario {idx + 1}: Evidence = {evidence}\")\n",
        "    for name, ve in inferences:\n",
        "        query_result = ve.query(variables=['match_outcome'], evidence=evidence)\n",
        "        print(f\"{name} Model Result:\\n{query_result}\")"
      ],
      "metadata": {
        "id": "mUVGPTQWUcqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter Sensitivity Analysis**\n",
        "\n",
        "> The impact of changes in key parameters on the inference results of the Bayesian Network models was analyzed. Parameters and their respective value ranges were defined, and inference was performed for varying parameter values. The resulting outputs were compared to evaluate the sensitivity of the models to parameter variations and to gain insights into their robustness and adaptability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cCwYJHboUvI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = ['Yellow_Cards_Home', 'Red_Cards_Home']\n",
        "values = [0, 1, 3]\n",
        "\n",
        "sensitivity_results = {}\n",
        "for param in parameters:\n",
        "    for value in values:\n",
        "        evidence = {param: value}\n",
        "        print(f\"\\nTesting {param} = {value}\")\n",
        "        for name, ve in inferences:\n",
        "            result = ve.query(variables=['match_outcome'], evidence=evidence)\n",
        "            print(f\"{name} Model: {result}\")\n",
        "            sensitivity_results[(name, param, value)] = result"
      ],
      "metadata": {
        "id": "vpbWzUPIU7ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Conclusion\n"
      ],
      "metadata": {
        "id": "q_a6dqlNt9XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Insights**\n",
        "\n",
        "The HillClimb Model emerged as the best-performing model, balancing structure complexity with predictive power.\n",
        "Offensive metrics, particularly Shots on Goal, have the strongest influence on match outcomes, while disciplinary metrics like Yellow/Red Cards negatively impact results.\n",
        "Bayesian Networks are effective in modeling the interplay of match statistics and outcomes, with structure-learning algorithms providing flexibility for automated insights.\n",
        "\n",
        "\\\n",
        "**Future Work**\n",
        "\n",
        "Explore additional features like player performance, match location, and weather conditions to enhance predictions.\n",
        "Test dynamic Bayesian networks (DBNs) to capture temporal dependencies, such as momentum shifts during a game.\n",
        "Use advanced scoring methods (e.g., BIC or AIC) to penalize overly complex models and avoid overfitting."
      ],
      "metadata": {
        "id": "Z9ORpCZAt_BE"
      }
    }
  ]
}